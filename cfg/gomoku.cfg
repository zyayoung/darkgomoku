[net]
batch=512
round=16

# The actual batchsize is decided by "round" in a batch.
# The batch is divided automatically into subbatch with maximum size "batch".
# Therefore, subdivision is decided automatically.
# subdivisions=1
height=13
width=13
nwin=5
channels=3
momentum=0.9
decay=1e-4

burn_in=1000
power=1

learning_rate=1e-2
# adam=1
max_batches = 2000000
policy=steps
steps=1200000,1800000
scales=.1,.1

[convolutional]
filters=128
size=3
stride=1
pad=1
activation=leaky
batch_normalize=1

[convolutional]
filters=128
size=3
stride=1
pad=1
activation=leaky
batch_normalize=1

[convolutional]
filters=128
size=3
stride=1
pad=1
activation=linear
batch_normalize=1

[shortcut]
from=-3
activation=leaky

[convolutional]
filters=128
size=3
stride=1
pad=1
activation=leaky
batch_normalize=1

[convolutional]
filters=128
size=3
stride=1
pad=1
activation=linear
batch_normalize=1

[shortcut]
from=-3
activation=leaky

[convolutional]
filters=128
size=3
stride=1
pad=1
activation=leaky
batch_normalize=1

[convolutional]
filters=128
size=3
stride=1
pad=1
activation=linear
batch_normalize=1

[shortcut]
from=-3
activation=leaky

#squeeze-n-excitation
[avgpool]

# squeeze ratio r=4 (recommended r=16)
[convolutional]
filters=32
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=128
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

[convolutional]
filters=128
size=3
stride=1
pad=1
activation=leaky
batch_normalize=1

[convolutional]
filters=128
size=3
stride=1
pad=1
activation=linear
batch_normalize=1

[shortcut]
from=-3
activation=leaky

[convolutional]
filters=128
size=3
stride=1
pad=1
activation=leaky
batch_normalize=1

[convolutional]
filters=128
size=3
stride=1
pad=1
activation=linear
batch_normalize=1

[shortcut]
from=-3
activation=leaky

[convolutional]
filters=1
size=1
stride=1
pad=1
temperature=0.05
activation=linear
